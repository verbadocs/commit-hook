#!/bin/bash

REPO_ROOT="$(git rev-parse --show-toplevel)"
cd "$REPO_ROOT" 

PROMPT_HISTORY_DIR="$REPO_ROOT/verba/prompt_history"
PROCESS_LOGS_SCRIPT="$REPO_ROOT/verba/process-logs.py"
MONITOR_SCRIPT="$REPO_ROOT/verba/monitor.py"

# Process new prompts.txt logs before commit
if [ -f "$PROCESS_LOGS_SCRIPT" ] && command -v python3 &> /dev/null; then
    echo "Processing new prompts.txt logs..."
    python3 "$PROCESS_LOGS_SCRIPT" --project-root .
fi

get_last_commit_timestamp() {
    if git rev-parse --verify HEAD >/dev/null 2>&1; then
        git log -1 --format="%ai" 2>/dev/null || echo "1970-01-01 00:00:00"
    else
        echo "1970-01-01 00:00:00"
    fi
}

LAST_COMMIT_TIME=$(get_last_commit_timestamp)
LAST_COMMIT_EPOCH=$(date -j -f "%Y-%m-%d %H:%M:%S %z" "$LAST_COMMIT_TIME" "+%s" 2>/dev/null || echo "0")

LAST_COMMIT_HASH=$(git rev-parse HEAD 2>/dev/null || echo "initial")
LAST_COMMIT_SHORT=$(git rev-parse --short HEAD 2>/dev/null || echo "initial")

mkdir -p "$PROMPT_HISTORY_DIR"

# Export database records for staged files to JSON
if [ -f "$MONITOR_SCRIPT" ] && command -v python3 &> /dev/null; then
    # Get list of staged files
    STAGED_FILES=$(git diff --cached --name-only)
    
    if [ ! -z "$STAGED_FILES" ]; then
        EXPORT_FILE="$PROMPT_HISTORY_DIR/db-export-$(date +%Y%m%d-%H%M%S).json"
        
        python3 -c "
import sqlite3
import sys
from datetime import datetime

staged_files = '''$STAGED_FILES'''.strip().split('\n')
staged_files = [f.strip() for f in staged_files if f.strip()]

if not staged_files:
    sys.exit(0)

conn = sqlite3.connect('verba/changes.db')
cursor = conn.cursor()

# Create placeholder list for SQL IN clause
placeholders = ','.join(['?' for _ in staged_files])
query = f'SELECT change_hash, filename, file_change, timestamp, prompt FROM code_changes WHERE filename IN ({placeholders}) AND is_committed = 0 ORDER BY timestamp ASC'

cursor.execute(query, staged_files)
matching_records = cursor.fetchall()

if matching_records:
    import json
    from datetime import datetime
    
    export_data = {
        'export_timestamp': datetime.now().isoformat(),
        'previous_commit_hash': '${LAST_COMMIT_HASH}',
        'previous_commit_short': '${LAST_COMMIT_SHORT}',
        'records': []
    }
    
    for change_hash, filename, file_change, timestamp, prompt in matching_records:
        record = {
            'change_hash': change_hash,
            'filename': filename,
            'file_change': file_change,
            'timestamp': timestamp,
            'prompt': prompt
        }
        export_data['records'].append(record)
    
    with open('$EXPORT_FILE', 'w') as f:
        json.dump(export_data, f, indent=2)

conn.close()
" 2>/dev/null || true

        # Add the export file to the commit if it was created
        if [ -f "$EXPORT_FILE" ]; then
            git add "$EXPORT_FILE"
        fi
    fi
fi

# --- ensure Verba artifacts are always staged ---
if [ -d "$REPO_ROOT/verba" ]; then
  # Stage everything under verba (quietly ignore missing paths)
   git add -f -A -- "$REPO_ROOT/verba" >/dev/null 2>&1 || true
fi
